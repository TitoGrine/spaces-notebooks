{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c33f6a3-69f0-400d-813f-3889b1c08d2b",
      "metadata": {},
      "source": "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/database.png\" />\n    </div>\n    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Integrating pandas with SingleStoreDB</h1>\n    </div>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "084012cb-df73-4887-a105-9d82e2755508",
      "metadata": {},
      "source": "This notebook will show how to move data from a pandas `DataFrame` into SingleStoreDB as well\nas downloading a SingleStoreDB query to a pandas `DataFrame`. It should be noted that this\nis only intended for relatively small data sets and to do processing that can't otherwise\nbe done in SingleStoreDB itself. Moving data to the client for processing should only be done\nwhen there is no other alternative in the database."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6f398e-e63c-4477-ab37-fbdcdd2d92f0",
      "metadata": {},
      "outputs": [],
      "source": "import ibis\nimport pandas as pd\nimport singlestoredb as s2\nimport sqlalchemy as sa"
    },
    {
      "cell_type": "markdown",
      "id": "3b83e421-0ae5-46b3-a64d-25d77de7da0c",
      "metadata": {},
      "source": "## Create a database\n\nWe need to create a database to work with in the following examples."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0ca2e7-52ec-4d97-b3e6-31ee4a2bd466",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nDROP DATABASE IF EXISTS pandas_integration;\n\nCREATE DATABASE IF NOT EXISTS pandas_integration;"
    },
    {
      "cell_type": "markdown",
      "id": "641ab7fd-a344-42f0-9cd9-755056374581",
      "metadata": {},
      "source": "<div class=\"alert alert-block alert-warning\">\n    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n    <div>\n        <p><b>Action Required</b></p>\n        <p>Make sure to select the <tt>pandas_integration</tt> database from the drop-down menu at the top of this notebook.\n        It updates the <tt>connection_url</tt> to connect to that database.</p>\n    </div>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "f094c4b2-dade-4432-9173-db590e7cb1dd",
      "metadata": {},
      "source": "## Database connections\n\nIn the notebooks environment, the connection string for the currently selected database is kept\nin the `connection_url` variable as well as the `SINGLESTOREDB_URL` environment variable.\nThe connection variables are accessed automatically within the SingleStoreDB Python packages\nso that you do not need connection parameters when connecting.\n\nIn the following sections, we will connect to SingleStoreDB using each of the packages and demonstrate\ntechniques for moving data between pandas and SingleStoreDB."
    },
    {
      "cell_type": "markdown",
      "id": "a6ffcb91-b70b-4175-bad7-7d087483e66b",
      "metadata": {},
      "source": "## The Iris data set\n\nWe'll be using the Iris data set for the following examples. This data set includes five columns: `sepal_length`,\n`sepal_width`, `petal_length`, `petal_width` and `class`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68129a0f-7bb7-4c1b-bdab-dbb5c8cb43a3",
      "metadata": {},
      "outputs": [],
      "source": "iris = pd.read_csv('https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/'\n                   'master/notebooks/integrating-with-pandas/data/iris.csv')\niris"
    },
    {
      "cell_type": "markdown",
      "id": "ff5ef64c-1046-4db0-bb96-dd976c42db39",
      "metadata": {},
      "source": "As you can see below, the first four columns are floats and the last column is a string \n(represented as an `object` in `DataFrame`s)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22392542-a71a-484b-9a3a-7e767d3bad07",
      "metadata": {},
      "outputs": [],
      "source": "iris.info()"
    },
    {
      "cell_type": "markdown",
      "id": "8ea1fa0c-247e-4aaa-98a1-0980921fcaf7",
      "metadata": {},
      "source": "## Moving data between SingleStoreDB and pandas `DataFrame`s\n\nMoving data from pandas `DataFrame`s to SingleStoreDB tables can be done in various ways from Python and even \nfrom each of the packages described here. This reference is to show the best techniques when using each \npackage.\n\nIt should be noted that moving data back-and-forth between pandas and SingleStoreDB should only be done when\nabsolutely needed since this can be a major bottleneck when working with and analyzing data. The hope is that\nthe features of SingleStoreDB are sufficient enough to alleviate the need to do much processing (if any) on \nthe client machine."
    },
    {
      "cell_type": "markdown",
      "id": "6d88c47a-7416-4566-b026-3e232afa7bc7",
      "metadata": {},
      "source": "### SingleStoreDB Python\n\nThe core library is the SingleStoreDB Python package. This is the package that all other SingleStoreDB\npackages are built on. To connect, simply call the `connect` function."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36c4a90-0aeb-402a-8454-764730ac4a08",
      "metadata": {},
      "outputs": [],
      "source": "s2_conn = s2.connect()"
    },
    {
      "cell_type": "markdown",
      "id": "9e54271f-e185-4965-bfaf-2639c304d503",
      "metadata": {},
      "source": "Since the core library is a fairly low-level interface to SingleStoreDB, most operations are done simply by sending\nSQL code."
    },
    {
      "cell_type": "markdown",
      "id": "46f57610-87bd-45c8-84bd-e77c9b313527",
      "metadata": {},
      "source": "#### Creating a table\n\nBecause we are using a low-level driver, creating a table is just done using SQL code. We'll use the information\nabout the data set above to construct a `CREATE TABLE` statement."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc666be-0ce7-46bb-9afe-19f3435f02ff",
      "metadata": {},
      "outputs": [],
      "source": "s2_cur = s2_conn.cursor()\ns2_cur.execute(r'''\n    CREATE TABLE IF NOT EXISTS iris (\n        sepal_length DOUBLE,\n        sepal_width DOUBLE,\n        petal_length DOUBLE,\n        petal_width DOUBLE,\n        class TEXT\n    )\n''')"
    },
    {
      "cell_type": "markdown",
      "id": "29c23826-1862-48ec-a837-a3e25ea52362",
      "metadata": {},
      "source": "#### Upload the data from a `DataFrame`\n\nNow that we have a table, we can populate it with data from the `DataFrame`. Again, we will use\nSQL statements to do this. The Python client can execute single SQL statements using the \n`execute` method as used above, but since we are uploading multiple rows of data it is better\nto use the `executemany` method since it is optimized for this purpose. "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7347b35-f82d-4bf4-bc7d-75a1ecd478e2",
      "metadata": {},
      "outputs": [],
      "source": "# Construct the list of column names\ncols = ', '.join(f'`{x}`' for x in iris.columns)\n\n# Construct a list of value placeholders for the INSERT statement\nvalues = ', '.join(['%s'] * len(iris.columns))\n\n# Get data as a list of tuples (not including the index)\ndata = list(iris.itertuples(index=False))\n\n# Execute the INSERT statement\ns2_cur.executemany(f'INSERT INTO iris({cols}) VALUES ({values})', data)"
    },
    {
      "cell_type": "markdown",
      "id": "f4846819-8043-4917-8b37-20ed4b4ab7ab",
      "metadata": {},
      "source": "We can select a sample of the rows to see that the data is now in SingleStoreDB."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801a8262-375a-40fc-8b2c-c15a8cee61a7",
      "metadata": {},
      "outputs": [],
      "source": "s2_cur.execute('SELECT * FROM iris LIMIT 10')\nfor row in s2_cur:\n    print(row)"
    },
    {
      "cell_type": "markdown",
      "id": "ff3de53a-76a8-44f5-a999-364692df4601",
      "metadata": {},
      "source": "#### Downloading the data to a `DataFrame`\n\nWe can download the data to a pandas `DataFrame` simply by selecting all columns of data, \nfetching all of the rows, and passing them to the `DataFrame` constructor."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9bf4d0-72be-4f70-9efa-129b041acba6",
      "metadata": {},
      "outputs": [],
      "source": "s2_cur.execute('SELECT * FROM iris')\n\n# Use the `description` attribute to get the column names\nnames = [x[0] for x in s2_cur.description]\n\ns2_iris_df = pd.DataFrame(list(s2_cur), columns=names)\ns2_iris_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2f9043-a940-43dc-974a-162e8bd2d576",
      "metadata": {},
      "outputs": [],
      "source": "s2_iris_df.info()"
    },
    {
      "cell_type": "markdown",
      "id": "809ccd96-f00b-49ff-b19c-48d9b85c7e3f",
      "metadata": {},
      "source": "Now that we have demonstrated uploading and downloading data from a pandas `DataFrame` using the\nSingleStoreDB Python client, we can drop the table and move on to SQLAlchemy."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef3cbb3-0f01-47c4-b27f-0dcf5729e7cd",
      "metadata": {},
      "outputs": [],
      "source": "s2_cur.execute('DROP TABLE IF EXISTS iris')"
    },
    {
      "cell_type": "markdown",
      "id": "75788e0a-df76-4be7-8ced-d01fdc37d7b6",
      "metadata": {},
      "source": "### SQLAlchemy\n\nIn addition to the core Python library, you can use SQLAlchemy to connect to SingleStoreDB. Typically, when \nusing SQLAlchemy, you would use the SQLAlchemy `create_engine` function to create an engine, then call `connect`\non the engine to create connections from a pool. The SingleStoreDB Python package also has a `create_engine`\nfunction that does the same thing, however, it extends the default ability by allow you to use the \n`SINGLESTOREDB_URL` environment variable as the connection string so that no parameters are needed for\n`create_engine` when used in the notebooks environment."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6ac741-7c01-4b34-95c8-59d97db70cab",
      "metadata": {},
      "outputs": [],
      "source": "sa_eng = s2.create_engine()\nsa_conn = sa_eng.connect()"
    },
    {
      "cell_type": "markdown",
      "id": "a0f0463a-9796-4f23-8081-6589fce6463d",
      "metadata": {},
      "source": "#### Uploading the data from a `DataFrame`\n\nUploading data from a `DataFrame` using SQLAlchemy is much easier than the lower-level Python library.\nThe pandas library itself has the ability to communicate with SingleStoreDB using a SQLAlchemy connection.\nIn this case, the `DataFrame` can create the table and populate it in one step using the `to_sql` method.\nThe `to_sql` method has various options to modify its behavior [documented on the pandas web site](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2c953c-09b9-4aa9-b491-751929322c19",
      "metadata": {},
      "outputs": [],
      "source": "iris.to_sql('iris', con=sa_conn, index=False, if_exists='replace')"
    },
    {
      "cell_type": "markdown",
      "id": "e4574813-c134-4c0a-8915-94c0c466863f",
      "metadata": {},
      "source": "We can verify the data is in SingleStoreDB with a simple `SELECT` statement."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6068dabe-5347-4449-926d-f52c0fb58b13",
      "metadata": {},
      "outputs": [],
      "source": "for row in sa_conn.execute('SELECT * FROM iris LIMIT 10'):\n    print(row)"
    },
    {
      "cell_type": "markdown",
      "id": "75b0a1ee-8c21-4b8a-856e-643485661bdf",
      "metadata": {},
      "source": "It is also possible to use SQLAlchemy expressions to query the table rather than raw SQL strings."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e13489-3d70-48c1-b17b-baebb0d0543c",
      "metadata": {},
      "outputs": [],
      "source": "# Create a metadata object for the database\ndb = sa.MetaData(bind=sa_eng)\nsa.MetaData.reflect(db)\n\n# Get the iris table from reflected data\nsa_iris = db.tables['iris']\n\n# Query the iris table\nquery = sa.select(sa_iris).limit(10)\n\n# Print results\nfor row in sa_conn.execute(query):\n    print(row)"
    },
    {
      "cell_type": "markdown",
      "id": "0db80753-98a7-4748-83fe-1190cd04415f",
      "metadata": {},
      "source": "#### Downloading the data to a `DataFrame`\n\nDownloading data to a pandas `DataFrame` is very simple. The result of the `execute` method can \nbe passed directly to the pandas `DataFrame` constructor."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6053dcf8-8774-4e73-a5d7-2a86bb597569",
      "metadata": {},
      "outputs": [],
      "source": "# Reset query to not include the limit\nquery = sa.select(sa_iris)\n\nsa_iris_df = pd.DataFrame(sa_conn.execute(query))\nsa_iris_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2a5a7b-a00d-48b5-8e8b-b19e08d5d792",
      "metadata": {},
      "outputs": [],
      "source": "sa_iris_df.info()"
    },
    {
      "cell_type": "markdown",
      "id": "a8977c4c-a0a8-4299-b7bb-d1c03e386169",
      "metadata": {},
      "source": "It is also possible to use `pd.read_sql` to bookend the use of `df.to_sql`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a200df59-efb2-42e6-bca5-1b115666fb2d",
      "metadata": {},
      "outputs": [],
      "source": "sa_iris_df = pd.read_sql(query, con=sa_conn)\nsa_iris_df"
    },
    {
      "cell_type": "markdown",
      "id": "c1dd2d93-f330-4e3a-b39a-32accc3d1a60",
      "metadata": {},
      "source": "Now that we have demonstrated using SQLAlchemy to upload and download pandas `DataFrames` we can drop \nthe table and move on to Ibis."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65eb886b-cdb6-4dae-bfaf-57a03d23cd51",
      "metadata": {},
      "outputs": [],
      "source": "sa_iris.drop()"
    },
    {
      "cell_type": "markdown",
      "id": "5693eca8-fa3c-46ad-94fb-73d02ca50478",
      "metadata": {},
      "source": "### Ibis (SingleStoreDB DataFrame)\n\nThe Ibis package allows you to treat tables in SingleStoreDB as `DataFrames`. The `DataFrame` expressions\nare used to build lazy expressions which generate SQL statements that get submitted to SingleStoreDB\nonly when you want to see the results of a query. Ibis using SQLAlchemy connections behind-the-scenes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686b8296-d50e-4a0b-b464-8b7807417f79",
      "metadata": {},
      "outputs": [],
      "source": "ibis_conn = ibis.singlestoredb.connect()"
    },
    {
      "cell_type": "markdown",
      "id": "254fd629-e321-434f-a257-ccc3874ac93b",
      "metadata": {},
      "source": "#### Uploading the data from a `DataFrame`\n\nIbis is intended for tight integration with pandas, so it is no surprise that uploading a \npandas `DataFrame` with Ibis is straight-forward. \n\nIf you are not familiar with Ibis, you may notice the `execute` call at the end of this cell.\nIbis creates expressions in memory on the client machine until a view of the data is \nexplicitly asked for. Once you explicitly ask for a query to be executed, it then generates\nand submits the SQL code for the expression behind-the-scenes.\n\nIn this case, the `ibis_iris` object is a `DataFrame`-like object that is lazily constructing\nthe requested expression until `execute` is called on it. In the case of this example, uploading\nand downloading "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca99ab2-bd93-41da-98f7-82c0d9934f43",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris = ibis_conn.create_table('iris', iris, force=True)\nibis_iris.limit(10).execute()"
    },
    {
      "cell_type": "markdown",
      "id": "49bb8050-a6f4-4a6d-b083-78b324ebc755",
      "metadata": {},
      "source": "It is also possible to insert the data from a `DataFrame` into an existing table using the `insert` method \nof the connection."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2841e5b-a8c9-41e3-a404-25d532e831d2",
      "metadata": {},
      "outputs": [],
      "source": "ibis_conn.insert('iris', iris)"
    },
    {
      "cell_type": "markdown",
      "id": "60cc0488-04ae-4c8b-9641-f6ef07133ac8",
      "metadata": {},
      "source": "You'll see that we now have 300 rows since we've inserted the data twice."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc49e1ac-25b3-49e6-a257-c2bf2ace5be7",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris.count().execute()"
    },
    {
      "cell_type": "markdown",
      "id": "125cc52e-37fe-4c9f-a573-c81f6363ff36",
      "metadata": {},
      "source": "One way to see the SQL that gets submitted during `execute` is to compile the expression\nand print it. Ibis also has a options to display SQL queries as they are submitted."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e4ccba-bec5-451c-9fd1-4b0ba87e5a74",
      "metadata": {},
      "outputs": [],
      "source": "print(ibis_iris.compile())"
    },
    {
      "cell_type": "markdown",
      "id": "11e9fa6e-bb62-488c-a530-835ebd41d323",
      "metadata": {},
      "source": "The information about the table can be retrieved much like in a local pandas `DataFrame`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfbb75b-ef83-4400-9909-348a12ac83cd",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris.info().execute()"
    },
    {
      "cell_type": "markdown",
      "id": "1bb2e364-b2c2-4c07-81ea-3e20dec1c723",
      "metadata": {},
      "source": "#### Downloading the data from a `DataFrame`\n\nThe output from evaluating Ibis expressions returns a `DataFrame`, so we have already demonstrated\ndownloading data, but here is the code again."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e14b01-9350-4560-81f9-cf68f4e105f5",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris_df = ibis_iris.execute()\nibis_iris_df"
    },
    {
      "cell_type": "markdown",
      "id": "e399be8a-6c5b-46cd-9c6b-7b4d02a36e57",
      "metadata": {},
      "source": "Ibis `Table`s also have a `to_pandas` method."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6623479-e200-4575-ae12-e415feb11237",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris.to_pandas()"
    },
    {
      "cell_type": "markdown",
      "id": "15feee4b-746b-4333-aabf-c8c061db9bb0",
      "metadata": {},
      "source": "If you do not have an Ibis object reference to a table already, you can get one using the `table` method\nor `tables` attribute of the Ibis connection."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1479ee9-10c6-47ac-b54e-3b3032c04949",
      "metadata": {},
      "outputs": [],
      "source": "# Use this version if the table name is not a valid Python variable name\nibis_iris = ibis_conn.table('iris')\n\n# This form can be used if the table name is a valid Python variable name\nibis_iris = ibis_conn.tables.iris"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec644ad-61e1-4ab7-9cb0-edf0ed2a1f81",
      "metadata": {},
      "outputs": [],
      "source": "ibis_iris.limit(10).execute()"
    },
    {
      "cell_type": "markdown",
      "id": "ef002757-d84c-4da2-a61c-48c424569261",
      "metadata": {},
      "source": "We have demonstrated both uploading and downloading pandas `DataFrames` using Ibis, so \nwe can drop the table now."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96373d5-651c-4047-bda7-3e49bf6edb7a",
      "metadata": {},
      "outputs": [],
      "source": "ibis_conn.drop_table('iris')"
    },
    {
      "cell_type": "markdown",
      "id": "d885cce5-1ae9-43e1-ae13-6b98c43ca23b",
      "metadata": {},
      "source": "### `%%sql` and `%sql` magic commands\n\nThe IPython interpreter can be extended with \"magic\" commands. The SingleStore Cloud notebook environment\nuses the [JupySQL](https://jupysql.ploomber.io/en/latest/quick-start.html) plugin for the `%sql`, `%%sql`,\nand `%sqlplot` commands. These work in conjunction with SQLAlchemy to allow you to type SQL code in\nthe notebook cells. They also have ways of integrating with pandas. The notebook environment automatically\nsets up the connection string for use in these commands."
    },
    {
      "cell_type": "markdown",
      "id": "a474e845-0039-4625-a250-2ec5cf0adaaa",
      "metadata": {},
      "source": "#### Creating a table\n\nCreating a table with the `%%sql` command is done simply by submitting the `CREATE TABLE` statement."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68222c9a-72ea-4917-9733-f621fd386bfb",
      "metadata": {},
      "outputs": [],
      "source": "%sql DROP TABLE iris;"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db10dca0-b2c5-43c9-8dcc-1f520fac7efb",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nCREATE TABLE IF NOT EXISTS iris (\n    sepal_length DOUBLE,\n    sepal_width DOUBLE,\n    petal_length DOUBLE,\n    petal_width DOUBLE,\n    class TEXT\n);"
    },
    {
      "cell_type": "markdown",
      "id": "f0f53739-6d19-4be8-ae08-c5dca35acfef",
      "metadata": {},
      "source": "#### Uploading the data from a `DataFrame`\n\nThe `%sql` command has options that allow you to upload data from a `DataFrame`. The `--persist` option\nwill create a table in the database and upload the data. The `--append` option will append data to an \nexisting table. In this case, the name used for the `DataFrame` variable is used for the table name\nin SingleStoreDB."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8839c965-880d-4909-88a0-b7fa8cb4723d",
      "metadata": {},
      "outputs": [],
      "source": "%sql --append --no-index iris"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11d4a80-9555-4067-9ada-2e4cf3ead35f",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nSELECT * FROM iris LIMIT 10;"
    },
    {
      "cell_type": "markdown",
      "id": "da339a4c-84a6-4f7d-a802-4fdd9f40d2b9",
      "metadata": {},
      "source": "#### Downloading the data from a `DataFrame`\n\nThere are a few ways of getting data from SingleStoreDB into a `DataFrame` using the SQL magic commands.\nThe first is to use the `%sql` command and convert the result manually."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4198f8b6-175d-4a16-963e-af7508e06487",
      "metadata": {},
      "outputs": [],
      "source": "out = %sql SELECT * FROM iris\nsql_iris_df = out.DataFrame()\nsql_iris_df"
    },
    {
      "cell_type": "markdown",
      "id": "c5978394-d1fe-4c24-a3bd-9b8ce9583dcc",
      "metadata": {},
      "source": "You can also pass the result of the query to the `DataFrame` constructor."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f008d44-efde-415c-b847-a7d351ac8157",
      "metadata": {},
      "outputs": [],
      "source": "sql_iris_df = pd.DataFrame(out)\nsql_iris_df"
    },
    {
      "cell_type": "markdown",
      "id": "01076d35-e406-453a-88c5-86f70b98e55e",
      "metadata": {},
      "source": "Finally, the output of the `%%sql` command can be stored to a variable which can then be\nconverted to a `DataFrame` in the same manner as above."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb36229-b336-462f-977f-36679d40f50f",
      "metadata": {},
      "outputs": [],
      "source": "%%sql result << \nSELECT * FROM iris;"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2094aa36-86bf-469b-9322-a9f94877f954",
      "metadata": {},
      "outputs": [],
      "source": "sql_iris_df = pd.DataFrame(result)\nsql_iris_df"
    },
    {
      "cell_type": "markdown",
      "id": "285fcfbb-5487-4195-9240-8af8462115fe",
      "metadata": {},
      "source": "##### Automatically return pandas `DataFrame`s\n\nThe other option for getting `DataFrame`s as the result of the SQL magic commands is to enable\nthe `SqlMagic.autopandas` option. This will cause all results from SQL magic commands to be\nconverted to `DataFrame`s without any intervention."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19aee9c-fe28-4c0b-a902-c094ea76cfd1",
      "metadata": {},
      "outputs": [],
      "source": "%config SqlMagic.autopandas = True"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6caba2b5-86b4-4abf-9063-e59bbac2aab3",
      "metadata": {},
      "outputs": [],
      "source": "out = %sql SELECT * FROM iris\nout"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129603ca-b982-4b77-9967-8c0fe0520f45",
      "metadata": {},
      "outputs": [],
      "source": "type(out)"
    },
    {
      "cell_type": "markdown",
      "id": "929f83b9-bb1c-4d6f-9a5e-fdfc8db0db7e",
      "metadata": {},
      "source": "Now that we have demonstrated uploading and downloading of `DataFrame`s using the SQL magic commands,\nwe can reset the configuration options and drop the table."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a14f6f-7afa-442c-8e58-c317bccdbcb9",
      "metadata": {},
      "outputs": [],
      "source": "%config SqlMagic.autopandas = False\n%sql DROP TABLE IF EXISTS iris"
    },
    {
      "cell_type": "markdown",
      "id": "6a832062-3934-4541-81b5-5c3cf0c117ac",
      "metadata": {},
      "source": "## Conclusion\n\nWe have shown how to upload and download data from a pandas `DataFrame` to and from SingleStoreDB\nusing the SingleStoreDB Python client, SQLAlchemy, and Ibis. These techniques should enable you to\nintegrate your pandas workflows with SingleStoreDB."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6f10e5-7605-485b-82f3-7a26a17ae912",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nDROP DATABASE IF EXISTS pandas_integration;"
    },
    {
      "cell_type": "markdown",
      "id": "dca02e68-11a8-46b9-b2eb-35f466d0c96e",
      "metadata": {},
      "source": "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
